# C++基础

# 常用设计模式

# 链表、栈、队列、树 

## Heap,Trie、BST、AVL、B+ Tree

# 进程间通信线程同步，多线程编程模型

# TCP/IP 四层模型

TCP/IP四层模型分别是应用层，传输层，网络层，网络接口层。

应用层是用户日常中接触到的，一些应用软件等就是处于应用层，应用层工作在操作系统的用户态，而其他层都工作在内核态，它只负责将数据发送给传输层，对于下层如何将数据传输给目的地是不感知的。

应用之间需要传输数据的时候会先将数据给传输层处理，传输层最常见的的两个协议就是TCP和UDP，这两个的主要区别是，一个是可靠性传输，一个是尽最大努力传输，TCP在建立连接时需要进行三次握手，保证了发送端和接收端都有收发能力才算建立链接成功，还存在一些传输控制技术，如拥塞控制，流量控制，校验技术，超时重传等来保证数据正确有序的到达。而UDP则没有这些保证，但是正因为没有这些技术的处理，导致它的传输效率会比TCP高很多，在不需要正确性保证的业务场景下，UDP会是更好的选择。当然，在日常中，一般是多个应用进程同时收发数据，这时我们使用端口来识别不同应用的数据，常见的端口22，就是远程服务器常用，80，是web服务器使用

在传输层，并不会将数据越过千山万水发送到目的端，这个过程是由网络层来完成，网络层的IP协议，给设备分配了IP地址，现在常用的有ipv4和ipv6，ipv4是32位的，通过网络号和主机号构成，网络号识别在哪个网络区域，主机号来识别同一个子网下的不同主机。网络号是通过子网掩码按位与运算得到。在网络环境中，存在很多设备，交换机，路由器，主机等，IP协议通过路由算法，找到对应的子网，再找到对应的主机，将数据通过网络发送子网。

在网络协议层，会将网络层加了IP头部的数据再包一层，加上MAC头部，才能发送到对应的设备，因为在以太网，是通过MAC地址来识别唯一的主机号的，而这个MAC地址是通过ARP地址解析协议来获得，获得了目的MAC地址和源MAC地址后，就可以将数据发送到目的地了。

#  Socket 网络编程

# ACID

**A(atomicity) :**事务中包含的各项操作在一次执行过程中，只允许出现以下两种状态之一：要么全做，要么不做，没有中间状态。

**C(consistency) :**在事务开始和完成时，数据库的数据都保持一致的状态，即事务的执行使数据库从一种正确状态转到另一种正确状态

**I(isolation) :**一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务是不能互相干扰的。

**D(durability) ：**一旦事务提交，那么它对数据库中的数据的改变就是永久性的，并不会被回滚。即使服务器系统崩溃或服务器宕机等故障。只要数据库重新启动，那么一定能够将其恢复到事务成功结束后的状态。

# CAP

**CAP定理**在分布式系统可能会出现故障，而为了系统的高可用，尽可能减少系统的停机时间，我们需要容忍一台节点或者多台节点出现问题不可用的情况，这样就需要引入冗余，而一旦添加了冗余机制，那么就会涉及到多副本的同步问题以及节点恢复问题。CAP定理抽象出三个概念

- 一致性(consistency)：各个数据副本数据的一致情况
- 可用性(Availability)：客户端的访问能得到回应
- 分区容忍性(Partition tolerance)：在一定时间内各个节点能够通信，达成数据一致

CAP定理指出，无法同时满足上述三个特性，因为如果要满足高一致性，那么必须等待数据副本的数据同步，那么可用性即降低，如果保证高可用性，那么就无法花太多时间等待数据副本同步。而如果想要同时满足CA，那么，节点之间需要互相通信以同步数据，这样分区容忍性无法保证，但是实际情况下，网络分区无法控制，所以，只能于CA中选择一个，或者CP，或者AP。

CAP中的C和ACID中的C是完全不一样的，ACID中的C是，事务将数据库从一个有效状态到另一个有效状态，这个有效状态即没有违反任何完整性约束条件。而在CAP中的C意味着事务的原子性和一致性，不让数据处于不一致的状态

# MVCC、OCC、2PL实现

事务处理的并发控制算法，主要分为乐观并发控制和悲观并发控制，乐观并发控制就是乐观的认为事务都是可以提交的，然后不考虑冲突的让事务执行完，然后在提交阶段对事务进行验证，判断是否真的无冲突，如果有冲突就根据规则abort或者retry

**乐观并发控制**主要通过时间戳的顺序来作为并发控制的主要依据，具体的OCC算法实现还是看个人，有些使用逻辑时钟，有些系统使用物理时钟，比如Silo这篇论文，主要就是在验证阶段对写集的每条记录加锁，然后再提交时验证是否有别的事务占用锁或者被别的事务修改过即可。物理时钟也存在一个问题，即时钟偏移问题，不同节点的时间可能存在一定的差异，但是现在很多时钟NTP组件可以做到ms级别的误差，应该也够用了。而Tictoc则是一篇PTOCC的论文，它的时间戳分配使用逻辑时钟，主要思路就是通过事务读写集维护每条数据的逻辑时钟区间，类似生命周期。PTOCC和LTOCC我感觉LTOCC就是注重于保证事务的偏序关系，而PTOCC在分配时按时间戳严格递增，时钟分配上会简单许多。OCC这种乐观的并发控制适合冲突少的负载，并发程度很高，但是缺点也很明显，存储开销很大，因为在OCC阶段需要将数据先存入本地空间，在验证通过之后再写入DBMS，并且不必要的事务重启也很多。

而**悲观并发控制**是悲观的认为事务都是存在冲突的，大多都是基于锁实现的，比如2PL，它在一个事务里面，分为加锁(lock)阶段和解锁(unlock)阶段,从 begin 开始，一直到 commit/rollback，都是加锁阶段，换句话说，整个事务期间，线程只抓取锁，不释放锁，直到事务结束，统一释放。2PL这种悲观的并发控制算法适合冲突多的负载，并发程度虽然相对较低，但是事务的abort率也会降低很多。

而**MVCC**是使用数据存多个版本的方式完成并发控制，在读时生成一致性读视图，用于支持读已提交和可重复读，在写入的适合创建新的物理版本，它有个好处就是写不会阻塞读，只读事务无需加锁即可获得一致性快照

快照隔离解决不了写偏斜问题，也就是两个事务，每个读取对方所写的一个数据项，但两个事务所写数据项集合交集为空，这种情况称为**写偏斜**（write skew）

# raft 共识 Paxos共识

## Raft是什么？和Paxos区别？

Raft其实就是分布式系统的共识算法，共识就是指多个参与者对于一件事达成一致，并且这个一致是最终的整个系统有效的，通常就是在多副本的情况下使用这种共识算法保证系统的副本一致性，备机在日志指令的驱动下，严格按照要求逐条执行日志中的指令。假定所有的状态都能按照相同的日志执行指令，则它们最终能达到相同的状态，这就能保证分布式系统状态的一致性。

Raft比paxos更好理解也更好实现，Raft将领导者变得更加重要，它把整个协议清楚的分为两个部分，一个是Leader选举，一个是日志复制，并且它利用日志的连续性做了一些简化，而paxos的日志是可以存在空隙的，因为他只接受比他接受过的提案编号更高的提案，而且不需要补全日志。

## Raft选举过程

选举的发生其实归根结底只有一种情况，节点超过了设定的时间没有收到心跳包，认为Leader已经挂了，这时节点会从Follower变为Candidate，Term+1，变成Candidate之后先投自己一票，向其他的所有节点发送请求投票的信息，在其他节点收到这个数据包后会判断自身是否已经投过票，并且会进行判断，这个Candidate的日志是否比我的更新，如果不是拥有更新日志的节点，那就不给它投票。为什么要有这个约束呢，是为了保证日志的安全性，防止拥有更旧日志的节点成为Leader导致新日志数据被覆盖，除了这个地方，保证日志安全性，Leader是不允许提交旧Term的日志的。然后当收到了一半以上的投票，那么Candidate成为Leader。

## Raft逻辑时钟会不会倒退

Term不允许倒退，Term只能是单调的，在Raft运行时，在节点超时后成为Candidate的时候Term会自增，其他时候Term的变化是通过收到比自己Term大的信息时，会更新自身Term，这些过程都是只增不降的。

## Raft日志同步

在存在Leader节点的时候，会响应客户端请求，将客户端传来的命令或数据附加到日志上，然后把日志增量向其他节点发送过去，来达到复制的效果，当这条数据被安全复制后，也就是超过半数节点都返回收到后，Leader就会提交这个日志，当然为了保证日志的安全性，防止有更旧日志的节点成为Leader，导致某些日志提交被覆盖，所以在选举时，日志更旧的节点是得不到拥有新日志的节点的投票的，并且Leader只能提交当前Term的数据，这两者保证不会出现提交的数据被覆盖的情况。当Leader提交了Log时，会覆盖那些没安全提交的日志。

## Raft脑裂

脑裂就是集群出现了网络分区的情况，这个情况其实解决方法可以很简单，如果不考虑客户端请求失败的情况，那么基础的Raft就可以实现，因为网络分区时，有少部分节点的分区无法达成共识，旧Leader就无法提交数据，就不会导致集群处于不一致的状态。也有一种方法可以解决这个问题，让集群尽量可以处理客户端请求，就是使用Leader lease，相当于给Leader一个可用范围，这个可用范围需要通过多数节点的心跳返回来续约，这样就可以让旧Leader在超过一定时间后，自动失效。

## 基于Raft的应用如何写raft，说出流程。

首先明确这个应用在哪部分需要使用Raft共识，一般来说都是某些需要保证分布式高度一致性的数据需要使用，为了解决副本一致性引入Raft，使用raft同步日志数据即可，因为写入了日志基本就保证数据不会丢失。

## 日志是如何提交的？

日志提交的过程说麻烦也不麻烦，但是有个前提，日志的提交只能由Leader发起，并且只能提交当前Term的日志，如果是之前的日志，那么只能同步，而不能提交。当客户端发起请求时，Leader将日志以及Term及index等信息发送给其他节点，如果发送的日志就是该节点需要的，那么就写入，这个节点就返回写入成功，否则就一直回退，回退到Leader和该节点可以匹配日志的位置，再将Leader后续的所有日志都同步过来，都同步完成之后，如果是当前Leader的Term就可以提交了。

## 日志回退情况（尚未提交日志的leader挂掉）？

日志回退的情况在集群都没有宕机的时候，可以是之前的那个leader的Term写的日志没有写到大多数节点上，这样在后面如果超时重新选举后，新Leader再写日志，发送给这个节点同步时就会覆盖这个日志，也就是相当于这个节点发生了日志回退。当然，已经提交的日志是不会发生日志回退情况的。

## 何为日志最新？与选主的关系？

我理解的日志最新就是最后一条日志的Term和Index的比较，Term大的日志就更新，Term如果相等那就比较index，index大的日志就更新。

在选主时，Follower只会给日志比自己更新的节点投票，这样可以保证不会出现Commit过的数据被覆盖。

## 快照是什么？快照流程。Why the design？

快照其实就是数据存储某一时刻的状态记录，这个记录包含了快照时刻的所有状态数据。

快照的流程，首先是客户端或者Leader发起创建快照的指令，创建快照时等于保留了快照时的所有数据，而且在实际应用中，日志是随着时间不断的增长的，但是又不能无限增长，所以我们需要抛弃日志中累计的过时的信息，因此，保存快照后，会将快照之前的日志数据完全删除。如果是Leader发起的快照指令，会将Leader自身的快照直接发送给Follower，如果Follower日志太落后，可以减少日志同步的时间，直接将日志同步到快照点。

## 为什么不设计成让follower自己做快照？

如果让Follower自己一段时间做一个快照，会存在问题，如果Follower自己当前的日志太落后了，打完快照之后还需要去Leader节点做日志的同步，如果让Leader节点发快照过来直接安装，那么也能减少日志同步的时间，并且，如果Follower自己打的快照比Leader打的快照更旧，那么还需要向Leader请求更新快照，就多次更新快照，性能也不如Leader直接让Follower更新快照好。

## 你这个发送快照的方式工程上是否可行？存在哪些问题？不可行的话请给出实现方法。（我说的类似AOF + RDB，尽量缩减checkpoints之间的时间）

对于稳定性和安全性来说，我觉得这种方式在工程上是可行的，但是对于性能上来说，可能就不太可行了，因为这个发送快照的方式过于频繁，如果发送快照需要

## 快照和日志同步同时发生怎么处理？不能用加锁的方式。

## Multi-Raft结构？

multi-raft其实就是在数据分片的集群中使用的共识协议，它主要是在系统将数据分片管理时使用，每个分片可以存在主分片和从分片，这些分片作为一个raft组，相当于有多少个分片就有多少个raft组。在全局看来就是整个系统中存在多个raft组。这个在Cockroach和Tidb上面都有。感觉还是挺难的，要考虑的东西比一个raft组更多，因为如果分片调度的话肯定涉及数据增长然后需要分裂组成更多raft组，分裂时的各种情况，比如Leader怎么处理，snapshot如何管理

# Gossip 传播

# 一致性模型

# 主备、多主、分片

# quorum 协议

# 2PC实现 3PC

# LSM 树

# Calvin、Spanner、Aurora、Neo4j

传统分布式事务处理使用到了锁来保证并发竞争的事务满足隔离级别的约束 ， 比如，序列化级别保证了事务是一个接一个运行的。而每个副本的执行顺序是无法预测的，但结果是可以预测的。  Calvin 这种确定性数据库的方案是让事务在每个副本上的执行顺序达到一致，那么执行结果也肯定是一致的。  这样做的好处是避免了众多事务之间的锁竞争，从而大大提高了高并发度事务的吞吐量。同时，节点崩溃不影响事务的执行。因为事务执行步骤已经分配，节点恢复后从失败处接着运行该事务即可，**这种模式使分布式事务的可用性也大大提高**。

# CRDT

# ZeroMQ 通信框架,其他消息队列

# Protocbuf

# 线程亲和性

# 数据库日志

# 项目

#### 面向跨区域高可用的 Coordination-Free 分布式事务处理机制

CCF 华为创新研究计划, 基于华为 openGauss 构建跨区域多主数据库。在熟悉 openGauss 数据库事务
处理代码的基础上解决以下问题：
• 多主架构：提高业务相应速度，将主从架构下的从节点提升为主节点，即多主全复制数据库。
• 强一致性：用 epoch 划分时间，保证 epoch 级别的强一致性；C++ 多线程编程。
• 跨节点写写冲突：使用确定性规则进行冲突合并，采用托马斯写原则；
• 高性能：使用 ZeroMQ 通信框架和 Epoch 内的流水线发送实现高效通信，Protocbuf 序列化工具传输的数据进行压缩，乐观并发控制 OCC 等技术实现高性能。

#### 基于无协调强一致性技术的多主架构云原生数据库 

CCF-华为胡杨林基金数据库专项利用上一个项目研究的无协调分布式事务处理机制设计成一个模块，
目的将数据库存储层计算层解耦，扩展成存算分离架构，以支持云原生数据库特性。
• 存算分离: 存储和计算节点完全解耦，事务处理层作为存储和计算的中间件协调事务执行。
• 可移植性: 通过编写接口适配如 KV，SQL，GQL 等多种数据模型；以及如 RocksDB，LevelDB 等多种存储引擎，大幅提高系统的可扩展性。

#### QStore 分布式分片数据库的复现及 Raft 保证高可用 

• 并发控制: 基于 QStore 论文实现事务并发控制协议，数据分片处理并将事务按数量划分成 epoch 后以epoch 粒度提交。
• 优先级队列: 将事务按涉及数据分片划分成子事务后进入优先级队列，后根据优先级队列以及多分区线程执行提交子事务，并保证事务的原子性，同时为多分区线程设置线程亲和性，降低调度开销。
• 系统高可用: 使用 Raft 共识实现数据库容错，支持选主、日志同步、状态持久化、日志恢复等功能。



